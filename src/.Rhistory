### More sample size means fewer models in the rashomon set for the same epsilon
# Function to generate synthetic data and evaluate subset selection models
generate_data <- function(sample_size, threshold, predictors = 30, n_var = 5,
seed = NULL, max_models = 10000) {
if (!is.null(seed)) set.seed(seed)
# Generate true coefficients - make them smaller and more numerous to increase overlap
true_coeffs <- rep(0, predictors)
# Make many variables weakly predictive (creates more model ambiguity)
true_coeffs[1:15] <- c(0.8, -0.6, 0.9, -0.4, 0.7, -0.5, 0.6, -0.3,
0.5, -0.7, 0.4, -0.8, 0.3, -0.4, 0.6)
# Generate predictors with substantial correlation structure
X <- matrix(rnorm(sample_size * predictors), nrow = sample_size, ncol = predictors)
# Create more extensive correlation structure
correlation_strength <- 0.6  # Increased correlation
for(i in 1:10) {
for(j in 1:3) {
if(i + j*5 <= predictors) {
X[, i + j*5] <- correlation_strength * X[, i] +
sqrt(1 - correlation_strength^2) * X[, i + j*5]
}
}
}
# Generate response with noise scaled to sample size to maintain signal-to-noise ratio
noise_sd <- 2 + 0.5 * log(sample_size/1000)  # Slightly increase noise with sample size
y <- X %*% true_coeffs + rnorm(sample_size, sd = noise_sd)
# Create all possible 5-variable combinations (sample more for larger datasets)
all_combinations <- combn(predictors, n_var, simplify = FALSE)
# Adjust sampling based on sample size - larger samples get more models tested
models_to_test <- min(length(all_combinations),
max_models * (1 + log10(sample_size/1000)))
if (length(all_combinations) > models_to_test) {
sampled_indices <- sample(length(all_combinations), models_to_test)
all_combinations <- all_combinations[sampled_indices]
}
cat("Evaluating", length(all_combinations), "models for sample size", sample_size, "\n")
# Evaluate each model with regularization for numerical stability
results <- map_dfr(all_combinations, function(vars) {
# Fit linear regression with slight regularization for stability
X_subset <- X[, vars, drop = FALSE]
# Add small ridge penalty for numerical stability
ridge_lambda <- 0.001
XtX <- t(X_subset) %*% X_subset + ridge_lambda * diag(ncol(X_subset))
Xty <- t(X_subset) %*% y
# Solve normal equations
coeffs <- solve(XtX, Xty)
intercept <- mean(y) - mean(X_subset %*% coeffs)
# Calculate predictions and RSS
y_pred <- intercept + X_subset %*% coeffs
rss <- sum((y - y_pred)^2)
tibble(
variables = list(vars),
rss = rss,
coefficients = list(as.numeric(coeffs)),
intercept = intercept
)
})
# Find best RSS
best_rss <- min(results$rss)
# Find models within threshold
threshold_rss <- best_rss * (1 + threshold)
rashomon_set <- results[results$rss <= threshold_rss, ]
# Create summary
summary_stats <- list(
sample_size = sample_size,
threshold = threshold,
total_models = nrow(results),
best_rss = best_rss,
threshold_rss = threshold_rss,
rashomon_set_size = nrow(rashomon_set),
rashomon_proportion = nrow(rashomon_set) / nrow(results)
)
return(list(
summary = summary_stats,
rashomon_models = rashomon_set,
all_results = results
))
}
# Function to analyze variable importance across Rashomon set
analyze_variable_importance <- function(rashomon_models, n_predictors = 30) {
# Count how often each variable appears
var_counts <- rep(0, n_predictors)
for(i in 1:nrow(rashomon_models)) {
vars <- rashomon_models$variables[[i]]
var_counts[vars] <- var_counts[vars] + 1
}
# Calculate coefficients summary for each variable
var_coeff_summary <- map_dfr(1:n_predictors, function(var_idx) {
# Find models containing this variable
models_with_var <- map_lgl(rashomon_models$variables, ~ var_idx %in% .x)
if (sum(models_with_var) == 0) {
return(tibble(variable = var_idx, count = 0, mean_coeff = NA,
sd_coeff = NA, min_coeff = NA, max_coeff = NA))
}
# Extract coefficients for this variable
coeffs <- map_dbl(which(models_with_var), function(model_idx) {
vars <- rashomon_models$variables[[model_idx]]
coeff_vec <- rashomon_models$coefficients[[model_idx]]
pos_in_model <- which(vars == var_idx)
coeff_vec[pos_in_model]
})
tibble(
variable = var_idx,
count = sum(models_with_var),
mean_coeff = mean(coeffs),
sd_coeff = sd(coeffs),
min_coeff = min(coeffs),
max_coeff = max(coeffs)
)
})
return(var_coeff_summary)
}
# Run simulation across different sample sizes and thresholds
run_rashomon_simulation <- function() {
# Simulation parameters - adjusted for better Rashomon effect demonstration
sample_sizes <- c(500, 2000, 8000, 20000)
thresholds <- c(0.001, 0.005, 0.01, 0.02)
# Run simulation
simulation_results <- expand_grid(
sample_size = sample_sizes,
threshold = thresholds
) %>%
mutate(
results = map2(sample_size, threshold,
~generate_data(.x, .y, seed = 42, max_models = 8000))
) %>%
mutate(
rashomon_size = map_dbl(results, ~.x$summary$rashomon_set_size),
rashomon_proportion = map_dbl(results, ~.x$summary$rashomon_proportion),
best_rss = map_dbl(results, ~.x$summary$best_rss),
total_models = map_dbl(results, ~.x$summary$total_models)
)
return(simulation_results)
}
# Visualize results
plot_rashomon_results <- function(sim_results) {
# Plot 1: Rashomon set size vs sample size
p1 <- sim_results %>%
ggplot(aes(x = sample_size, y = rashomon_size, color = factor(threshold))) +
geom_line(size = 1.2) +
geom_point(size = 3) +
scale_x_log10(labels = scales::comma) +
scale_color_viridis_d(name = "Threshold") +
labs(
title = "Rashomon Set Size vs Sample Size",
subtitle = "Larger samples â†’ More models within performance threshold",
x = "Sample Size (log scale)",
y = "Number of Models in Rashomon Set"
) +
theme_minimal()
# Plot 2: Proportion in Rashomon set
p2 <- sim_results %>%
ggplot(aes(x = sample_size, y = rashomon_proportion, color = factor(threshold))) +
geom_line(size = 1.2) +
geom_point(size = 3) +
scale_x_log10(labels = scales::comma) +
scale_y_continuous(labels = scales::percent) +
scale_color_viridis_d(name = "Threshold") +
labs(
title = "Proportion of Models in Rashomon Set",
x = "Sample Size (log scale)",
y = "Proportion of Models Within Threshold"
) +
theme_minimal()
return(list(size_plot = p1, proportion_plot = p2))
}
# Example of extracting different "stories" from Rashomon set
extract_different_stories <- function(rashomon_models, n_stories = 3) {
# Sample different models from Rashomon set
if (nrow(rashomon_models) < n_stories) {
selected_indices <- 1:nrow(rashomon_models)
} else {
selected_indices <- sample(nrow(rashomon_models), n_stories)
}
stories <- map(selected_indices, function(idx) {
vars <- rashomon_models$variables[[idx]]
coeffs <- rashomon_models$coefficients[[idx]]
intercept <- rashomon_models$intercept[[idx]]
rss <- rashomon_models$rss[[idx]]
# Create equation string
terms <- paste0(round(coeffs, 2), "*x", vars)
equation <- paste("y =", round(intercept, 2), "+", paste(terms, collapse = " + "))
equation <- str_replace_all(equation, "\\+ -", "- ")
list(
model_index = idx,
variables = vars,
equation = equation,
rss = rss
)
})
return(stories)
}
# Run the simulation
cat("Running Rashomon Effect Simulation...\n")
results <- run_rashomon_simulation()
# Display summary with better formatting
cat("\nRashomon Set Sizes by Sample Size and Threshold:\n")
results %>%
select(sample_size, threshold, total_models, rashomon_size, rashomon_proportion) %>%
arrange(sample_size, threshold) %>%
mutate(
rashomon_proportion = paste0(round(rashomon_proportion * 100, 1), "%")
) %>%
print()
# Show the key insight: larger samples -> larger Rashomon sets
cat("\nKey Insight - Rashomon Set Size vs Sample Size:\n")
results %>%
group_by(threshold) %>%
arrange(sample_size) %>%
select(threshold, sample_size, rashomon_size) %>%
print()
# Create plots
plots <- plot_rashomon_results(results)
print(plots$size_plot)
print(plots$proportion_plot)
# Example: Show different "stories" for largest sample size
largest_sample_result <- results %>%
filter(sample_size == max(sample_size), threshold == 0.01) %>%
pull(results) %>%
.[[1]]
stories <- extract_different_stories(largest_sample_result$rashomon_models, 3)
cat("\nExample of different 'stories' from the same data:\n")
for (i in seq_along(stories)) {
cat("Story", i, ":", stories[[i]]$equation, "\n")
cat("RSS:", round(stories[[i]]$rss, 2), "\n")
cat("Variables used: x", paste(stories[[i]]$variables, collapse = ", x"), "\n\n")
}
# Analyze variable importance
var_importance <- analyze_variable_importance(largest_sample_result$rashomon_models)
cat("Variable importance in Rashomon set (top 10):\n")
var_importance %>%
filter(count > 0) %>%
arrange(desc(count)) %>%
head(10) %>%
print()
View(results)
head(results)
write.csv(results, file = '../data/results.csv')
write.csv(results, file = '../data/results.csv')
setwd("~/Desktop/many_analysts")
write.csv(results, file = '../data/results.csv')
write.csv(results, file = '../data/results.csv')
write.csv(results, file = '../data/results.csv', row.names = FALSE)
getwd()
# save as csv
write.csv(results, file = '../data/results.csv', row.names = FALSE)
setwd("~/Desktop/many_analysts/src")
# save as csv
write.csv(results, file = '../data/results.csv', row.names = FALSE)
typeof(results)
# save as csv
write.csv(results, file = '../data/results.csv', row.names = FALSE)
View(results)
# Rashomon Effect Simulation
# Based on Breiman (2001): "Statistical Modeling: The Two Cultures"
library(leaps)  # for regsubsets (best subset selection)
library(data.table)
set.seed(123)  # for reproducibility
# Generate population data
n_pop <- 10000
n_vars <- 30
# Generate covariates from standard normal
X_pop <- matrix(rnorm(n_pop * n_vars), nrow = n_pop, ncol = n_vars)
colnames(X_pop) <- paste0("X", 1:n_vars)
# Generate true coefficients (only 5 are non-zero for true model)
true_coefs <- rep(0, n_vars)
true_vars <- sample(1:n_vars, 5)
true_coefs[true_vars] <- runif(5, min = -5, max = 5)
# Generate outcome with noise
y_pop <- X_pop %*% true_coefs + rnorm(n_pop, sd = 2)
# Combine into data frame
pop_data <- data.frame(y = y_pop, X_pop)
# Sample sizes to test
sample_sizes <- c(1000, 2000, 5000, 10000, 15000, 20000)
sample_sizes <- c(100)
# Function to find models within threshold of best model
find_rashomon_set <- function(data, n_vars_select = 5, threshold = 0.1) {
# Fit best subset regression
fit <- regsubsets(y ~ ., data = data, nvmax = n_vars_select,
method = "exhaustive", really.big = TRUE)
# Get RSS for all models with exactly n_vars_select variables
summ <- summary(fit)
# For 5-variable models, we need to explore all combinations
# Since regsubsets only gives us the best model, we need to manually check others
# Get all possible 5-variable combinations
all_vars <- names(data)[-1]  # exclude 'y'
all_combos <- combn(all_vars, n_vars_select)
# Calculate RSS for each combination
rss_values <- numeric(ncol(all_combos))
for (i in 1:ncol(all_combos)) {
vars <- all_combos[, i]
formula <- as.formula(paste("y ~", paste(vars, collapse = " + ")))
lm_fit <- lm(formula, data = data)
rss_values[i] <- sum(residuals(lm_fit)^2)
}
# Find minimum RSS
min_rss <- min(rss_values)
# Count models within threshold
threshold_rss <- min_rss * (1 + threshold)
n_models_in_set <- sum(rss_values <= threshold_rss)
return(n_models_in_set)
}
# Run simulation for each sample size
results <- data.table(sample_size = integer(),
n_models_rashomon = integer())
cat("Running Rashomon Effect simulation...\n")
for (n in sample_sizes) {
cat(sprintf("Sample size: %d\n", n))
# Take sample from population
if (n <= n_pop) {
sample_idx <- sample(1:n_pop, n, replace = FALSE)
} else {
# For samples larger than population, sample with replacement
sample_idx <- sample(1:n_pop, n, replace = TRUE)
}
sample_data <- pop_data[sample_idx, ]
# Find Rashomon set size
n_models <- find_rashomon_set(sample_data)
# Store results
results <- rbind(results,
data.table(sample_size = n,
n_models_rashomon = n_models))
cat(sprintf("  Models within 1%% of best: %d\n", n_models))
}
# Save results
write.csv(results, file = '../data/result.csv', row.names = FALSE)
cat("\nResults saved to '../data/result.csv'\n")
print(results)
# Function to find models within threshold of best model
find_rashomon_set <- function(data, n_vars_select = 5, threshold = 0.5) {
# Fit best subset regression
fit <- regsubsets(y ~ ., data = data, nvmax = n_vars_select,
method = "exhaustive", really.big = TRUE)
# Get RSS for all models with exactly n_vars_select variables
summ <- summary(fit)
# For 5-variable models, we need to explore all combinations
# Since regsubsets only gives us the best model, we need to manually check others
# Get all possible 5-variable combinations
all_vars <- names(data)[-1]  # exclude 'y'
all_combos <- combn(all_vars, n_vars_select)
# Calculate RSS for each combination
rss_values <- numeric(ncol(all_combos))
for (i in 1:ncol(all_combos)) {
vars <- all_combos[, i]
formula <- as.formula(paste("y ~", paste(vars, collapse = " + ")))
lm_fit <- lm(formula, data = data)
rss_values[i] <- sum(residuals(lm_fit)^2)
}
# Find minimum RSS
min_rss <- min(rss_values)
# Count models within threshold
threshold_rss <- min_rss * (1 + threshold)
n_models_in_set <- sum(rss_values <= threshold_rss)
return(n_models_in_set)
}
# Run simulation for each sample size
results <- data.table(sample_size = integer(),
n_models_rashomon = integer())
cat("Running Rashomon Effect simulation...\n")
for (n in sample_sizes) {
cat(sprintf("Sample size: %d\n", n))
# Take sample from population
if (n <= n_pop) {
sample_idx <- sample(1:n_pop, n, replace = FALSE)
} else {
# For samples larger than population, sample with replacement
sample_idx <- sample(1:n_pop, n, replace = TRUE)
}
sample_data <- pop_data[sample_idx, ]
# Find Rashomon set size
n_models <- find_rashomon_set(sample_data)
# Store results
results <- rbind(results,
data.table(sample_size = n,
n_models_rashomon = n_models))
cat(sprintf("  Models within 1%% of best: %d\n", n_models))
}
# Save results
write.csv(results, file = '../data/result.csv', row.names = FALSE)
cat("\nResults saved to '../data/result.csv'\n")
print(results)
seq(from=100, to=2000, by=100)
sample_sizes <- seq(from=100, to=2000, by=100)
library(leaps)  # for regsubsets (best subset selection)
library(data.table)
set.seed(123)  # for reproducibility
# Generate population data
n_pop <- 10000
n_vars <- 30
# Generate correlated covariates (this creates more similar models)
# Create correlation matrix with some structure
Sigma <- matrix(0.3, n_vars, n_vars)  # baseline correlation
diag(Sigma) <- 1
# Add some stronger correlations between groups of variables
# Group 1: variables 1-5
Sigma[1:5, 1:5] <- 0.7
diag(Sigma) <- 1
# Group 2: variables 6-10
Sigma[6:10, 6:10] <- 0.6
diag(Sigma) <- 1
# Group 3: variables 11-15
Sigma[11:15, 11:15] <- 0.65
diag(Sigma) <- 1
# Generate correlated predictors
X_pop <- mvrnorm(n_pop, mu = rep(0, n_vars), Sigma = Sigma)
library(MASS)
# Generate correlated predictors
X_pop <- mvrnorm(n_pop, mu = rep(0, n_vars), Sigma = Sigma)
colnames(X_pop) <- paste0("X", 1:n_vars)
# Generate coefficients - many small effects rather than few large ones
# This creates a situation where many subsets can perform similarly
true_coefs <- rnorm(n_vars, mean = 0, sd = 1)
# Make some coefficients larger
important_vars <- sample(1:n_vars, 10)
true_coefs[important_vars] <- true_coefs[important_vars] * 2
# Generate outcome with moderate noise
# Higher noise relative to signal creates more model uncertainty
signal <- X_pop %*% true_coefs
noise_sd <- sd(signal) * 0.8  # noise is 80% of signal SD
y_pop <- signal + rnorm(n_pop, sd = noise_sd)
# Combine into data frame
pop_data <- data.frame(y = y_pop, X_pop)
View(pop_data)
# Sample sizes to test
sample_sizes <- seq(from=100, to=2000, by=100)
sample_sizes = c(100)
# Function to find models within threshold of best model
find_rashomon_set <- function(data, n_vars_select = 5, threshold = 0.01) {
# Get all possible 5-variable combinations
all_vars <- names(data)[-1]  # exclude 'y'
all_combos <- combn(all_vars, n_vars_select)
# Calculate RSS for each combination
n_models <- ncol(all_combos)
rss_values <- numeric(n_models)
cat(sprintf("  Evaluating %d possible 5-variable models...\n", n_models))
# Progress indicator for large computations
pb <- txtProgressBar(min = 0, max = n_models, style = 3)
for (i in 1:n_models) {
vars <- all_combos[, i]
formula <- as.formula(paste("y ~", paste(vars, collapse = " + ")))
lm_fit <- lm(formula, data = data)
rss_values[i] <- sum(residuals(lm_fit)^2)
setTxtProgressBar(pb, i)
}
close(pb)
# Find minimum RSS
min_rss <- min(rss_values)
# Count models within threshold
threshold_rss <- min_rss * (1 + threshold)
n_models_in_set <- sum(rss_values <= threshold_rss)
# Also calculate for different thresholds to see distribution
thresholds <- c(0.01, 0.02, 0.05, 0.10, 0.20, 0.50)
counts <- sapply(thresholds, function(t) {
sum(rss_values <= min_rss * (1 + t))
})
cat(sprintf("  Models within various thresholds of best:\n"))
for (i in seq_along(thresholds)) {
cat(sprintf("    Within %d%%: %d models\n", thresholds[i]*100, counts[i]))
}
return(list(
n_models_1pct = n_models_in_set,
all_counts = data.frame(
threshold = thresholds,
n_models = counts
),
min_rss = min_rss,
rss_values = rss_values
))
}
# Run simulation for each sample size
results <- data.table(sample_size = integer(),
n_models_rashomon = integer())
# Store detailed results for analysis
detailed_results <- list()
cat("Running Rashomon Effect simulation...\n")
cat("Data generated with correlated predictors and distributed effects\n\n")
for (n in sample_sizes) {
cat(sprintf("Sample size: %d\n", n))
# Take sample from population
if (n <= n_pop) {
sample_idx <- sample(1:n_pop, n, replace = FALSE)
} else {
# For samples larger than population, sample with replacement
sample_idx <- sample(1:n_pop, n, replace = TRUE)
}
sample_data <- pop_data[sample_idx, ]
# Find Rashomon set size
rashomon_results <- find_rashomon_set(sample_data)
# Store results
results <- rbind(results,
data.table(sample_size = n,
n_models_rashomon = rashomon_results$n_models_1pct))
# Store detailed results
detailed_results[[as.character(n)]] <- rashomon_results
cat("\n")
}
# Save main results
write.csv(results, file = '../data/result.csv', row.names = FALSE)
# Save detailed results for different thresholds
all_threshold_results <- data.table()
for (n in names(detailed_results)) {
temp <- detailed_results[[n]]$all_counts
temp$sample_size <- as.integer(n)
all_threshold_results <- rbind(all_threshold_results, temp)
}
write.csv(all_threshold_results, file = '../data/detailed_results.csv', row.names = FALSE)
cat("\nResults saved to '../data/result.csv' and '../data/detailed_results.csv'\n")
print(results)
# Show correlation structure impact
cat("\n\nCorrelation structure of predictors:\n")
cat("Average correlation: ", mean(Sigma[upper.tri(Sigma)]), "\n")
cat("This creates many competing models with similar performance\n")
View(results)
